# Models
## Logistic Regression Classification
This model works a lot like linear regression, but with a sine function applied. The result is that any output will be on the scale of 0-1 and its overall shape will resemble a sine wave. A dataset can have one or more independent variables to determine the likelihood of the dependent variable.
## K-Nearest Neighbors Classification
This model is kind of like using a scatter plot with a threshold to determine whether or not something is likely to occur. Values are grouped by how often they produce the desired outcome, and any values to be evaluated are grouped with the closest values. I believe multiple independent variables/inputs can be used.
## Support Vector Machine
This is basically the same thing as the linear regression version, but now it is predicting the likelihood of an outcome occuring.
## Kernel Support Vector Machine
This is similar to Support Vector Machines, but due to its ability to add a third dimension to a dataset it can handle non-linear datasets. A non-linear dataset could be a scatterplot without an obvious pattern.
## Naive Bayes
This uses a few factors to compute the probability of an outcome occuring. The factors consider total possible outcomes, total possible samples, and a radius of relevant and irrelevant samples.
## Decision Tree Classifier
Basically the same thing as its Regression version, but determining a likelihood instead of predicting a precise value.
## Random Forest Classifier
Basically the same thing as its Regression version, but determining a likelihood instead of predicting a precise value.

# Metrics
## Accuracy Score
## Confusion Matrix

# <p align=center>References</p>

[Machine Learning A-Z by Kirill Eremenko, Hadelin de Ponteves, and Ligency Team](https://www.udemy.com/course/machinelearning/learn/lecture/19596438?start=1#overview)
